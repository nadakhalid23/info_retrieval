{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115a64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\new\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9f8d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2083e60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Link</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>تيرتل تري لابز للتقنيات الحيويه تحصل علي تمويل...</td>\n",
       "      <td>جاءت جوله التمويل التمهيديه الاولي للشركه بقيا...</td>\n",
       "      <td>https://ryadiybusiness.com/%d8%aa%d9%8a%d8%b1%...</td>\n",
       "      <td>ريادة أعمال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تعد دراسه الجدوي لمشروعك الناشء 6 عوامل اساسيه</td>\n",
       "      <td>تعد دراسه الجدوي متطلبا اساسيا لنجاح اي مشروع ...</td>\n",
       "      <td>https://ryadiybusiness.com/%d9%83%d9%8a%d9%81-...</td>\n",
       "      <td>ريادة أعمال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>منح جاءزه نوبل الطب لاكتشاف مجال التهاب الكبد ...</td>\n",
       "      <td>وقالت لجنه نوبل للمره الاولي التاريخ يمكن الان...</td>\n",
       "      <td>https://arabic.sputniknews.com/science/2020100...</td>\n",
       "      <td>علوم وتكنولوجيا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هيءه الغذاء والدواء الامريكيه لقاح مودرنا حقق ...</td>\n",
       "      <td>نشرت الوكاله وثاءق علي الانترنت اعدها موظفوها ...</td>\n",
       "      <td>https://arabic.sputniknews.com/world/202012151...</td>\n",
       "      <td>علوم وتكنولوجيا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حظر بروتين الجهاز المناعي يساعد علي محاربه عدو...</td>\n",
       "      <td>دراسه حديثه قام فريق العلماء جونز هوبكنز ميديس...</td>\n",
       "      <td>https://arabic.rt.com//technology/1161864-%D8%...</td>\n",
       "      <td>علوم وتكنولوجيا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  تيرتل تري لابز للتقنيات الحيويه تحصل علي تمويل...   \n",
       "1     تعد دراسه الجدوي لمشروعك الناشء 6 عوامل اساسيه   \n",
       "2  منح جاءزه نوبل الطب لاكتشاف مجال التهاب الكبد ...   \n",
       "3  هيءه الغذاء والدواء الامريكيه لقاح مودرنا حقق ...   \n",
       "4  حظر بروتين الجهاز المناعي يساعد علي محاربه عدو...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  جاءت جوله التمويل التمهيديه الاولي للشركه بقيا...   \n",
       "1  تعد دراسه الجدوي متطلبا اساسيا لنجاح اي مشروع ...   \n",
       "2  وقالت لجنه نوبل للمره الاولي التاريخ يمكن الان...   \n",
       "3  نشرت الوكاله وثاءق علي الانترنت اعدها موظفوها ...   \n",
       "4  دراسه حديثه قام فريق العلماء جونز هوبكنز ميديس...   \n",
       "\n",
       "                                                Link         Category  \n",
       "0  https://ryadiybusiness.com/%d8%aa%d9%8a%d8%b1%...      ريادة أعمال  \n",
       "1  https://ryadiybusiness.com/%d9%83%d9%8a%d9%81-...      ريادة أعمال  \n",
       "2  https://arabic.sputniknews.com/science/2020100...  علوم وتكنولوجيا  \n",
       "3  https://arabic.sputniknews.com/world/202012151...  علوم وتكنولوجيا  \n",
       "4  https://arabic.rt.com//technology/1161864-%D8%...  علوم وتكنولوجيا  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kaggle.com/datasets/asmaaabdelwahab/arabic-news-dataset\n",
    "data = pd.read_csv(\"processed_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606e9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Title',inplace=True,axis=1)\n",
    "data.drop('Link',inplace=True,axis=1)\n",
    "data.drop('Category',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8597ebc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822c99fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8386, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cabf0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "dupl = data[data.duplicated()]\n",
    "if len(dupl)>0:\n",
    "    data=data.drop_duplicates()\n",
    "    print(len(dupl))\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbf74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    if not isinstance(text, (str, bytes)):\n",
    "        # If input is not a string or bytes-like object, convert it to string\n",
    "        text = str(text)\n",
    "    \n",
    "    if text.startswith(\"ال\"):\n",
    "        text = text[2:]\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"[ًٌٍَُِّْ]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdfd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arabic_Remove_stop(words):\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    preprocessed_text=[]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    preprocessed_text = words\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(words):\n",
    "\n",
    "\n",
    "    cleaned_words = [word for word in words if word not in string.punctuation]\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0441c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(words):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbefadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    out = remove_punctuation(words)\n",
    "    out = preprocess_arabic_Remove_stop(out)\n",
    "    out = lemma(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a2f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(documents):\n",
    "    inverted_index = dict()\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        doc = normalize_arabic(doc)\n",
    "        terms = preprocess_text(doc)\n",
    "\n",
    "        for term in terms:\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = []\n",
    "            if doc_id not in inverted_index[term]:\n",
    "                inverted_index[term].append(doc_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b033379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(data['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4ad3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, inverted_index, documents):\n",
    "    query = normalize_arabic(query)\n",
    "    query = preprocess_text(query)\n",
    "    relevant_doc_ids = list(range(len(documents)))\n",
    "\n",
    "    ok = 0\n",
    "    print(query)\n",
    "\n",
    "    for term in query:\n",
    "        relevant_docs = []\n",
    "        if term in inverted_index:\n",
    "            p1 = 0\n",
    "            p2 = 0\n",
    "            while p1 < len(relevant_doc_ids) and p2 < len(inverted_index[term]):\n",
    "                if relevant_doc_ids[p1] < inverted_index[term][p2]:\n",
    "                    p1 += 1\n",
    "                elif relevant_doc_ids[p1] > inverted_index[term][p2]:\n",
    "                    p2 += 1\n",
    "                else:\n",
    "                    relevant_docs.append(relevant_doc_ids[p1])\n",
    "                    p1 += 1\n",
    "                    p2 += 1\n",
    "            relevant_doc_ids = relevant_docs\n",
    "    return relevant_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d957fb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['جونز', 'هوبكنز']\n",
      "['جونز', 'هوبكنز']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4       دراسه حديثه قام فريق العلماء جونز هوبكنز ميديس...\n",
       "69      الان بدا توزيع لقاحات عديده اماكن مختلفه العال...\n",
       "221     تمكن فريق الباحثين مختبر الفيزياء التطبيقيه ال...\n",
       "702     ظل الزياده الكبيره تكاليف الامدادات والشحن وال...\n",
       "999     جاء دراسه جديده صدرت قبيل المنتدي العالمي الاو...\n",
       "2053    يستعد مارسيل مونستر مءسس صندوق جريتي حاليا لتق...\n",
       "2872    يتطرق معرض ومءتمر الصحه العربي اراب هيلث يعد ا...\n",
       "3017    ولاحظ العلماء ان النجوم تتجاوز كتلتها كتله الش...\n",
       "3063    اعلنت شركه iCare Solutions اطلاق اول منصه لتتب...\n",
       "4490    واكدت الدراسه المنشوره مءخرا مجله Obesity ان ف...\n",
       "4641    ويكشف التقرير المعنون ماساه مهمله العبء العالم...\n",
       "5560    الهمت المعادله الشهيره المستخدمه البحث حياه فض...\n",
       "5690    يقول باحثون امريكيون ان فيروس كورونا ربما يتلا...\n",
       "5970    وصل خبير علم الكونيات تومي تينكانين جامعه جونز...\n",
       "6086    واضافت المنظمه ان السلطات البريطانيه قالت انه ...\n",
       "6358    وقال موقع ميديكال اكسبرس ان باحثين بجامعه جونز...\n",
       "6503    وبحسب الباحثين معهد القياسات الصحيه والتقييم ا...\n",
       "7550    واوضحت الدراسه المنشوره عبر موقع تايمز نيوز نا...\n",
       "7647    واوضح نيتيسوف يتم البحث الوقت الحالي بشان مشار...\n",
       "7804    وجد باحثون ان البشر مثلهم مثل انواع الاسماك وا...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('جونز هوبكنز',inverted_index,data['Content'])\n",
    "res=data.iloc[search('جونز هوبكنز',inverted_index,data['Content'])]\n",
    "res.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2b1c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03bd25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(query):\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "    X = vectorizer.fit_transform(data['Content'])\n",
    "\n",
    "\n",
    "    # Tokenize and vectorize the query\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    query_vector_dense = query_vector.toarray()\n",
    "\n",
    "    # Calculate cosine similarity between query and documents\n",
    "    cosine_similarities = cosine_similarity(query_vector_dense, X)\n",
    "\n",
    "    # Add cosine similarities to DataFrame\n",
    "    data['cosine_similarity'] = cosine_similarities[0]\n",
    "\n",
    "    # Sort DataFrame by cosine similarity\n",
    "    df_sorted = data.sort_values(by='cosine_similarity', ascending=False)\n",
    "\n",
    "    # Filter out documents with cosine similarity greater than 0\n",
    "    similar_documents = df_sorted[df_sorted['cosine_similarity'] > 0.0]\n",
    "\n",
    "    similar_document_ids = similar_documents.iloc[:, 0]\n",
    "\n",
    "    return similar_document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae5898ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bisect\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter.filedialog import askopenfilename, askopenfilenames\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk , Image\n",
    "import os\n",
    "import ast\n",
    "root=Tk()\n",
    "root.geometry('750x600')\n",
    "root.resizable(False,False)\n",
    "root.title('IR')\n",
    "root.config(background='#ffffff')\n",
    "img=Image.open(\"D://src_ir.png\")\n",
    "img_resize=img.resize((10,10))\n",
    "icon=ImageTk.PhotoImage(img_resize)\n",
    "root.iconphoto(False,icon)\n",
    "docs=StringVar()\n",
    "ranked_docs=StringVar()\n",
    "query=StringVar()\n",
    "def ranking():\n",
    "    docs.set(\"\")\n",
    "    query_str = query.get()\n",
    "    cosine_similarities = cosine(query_str)\n",
    "    docs.set(cosine_similarities)\n",
    "\n",
    "def search2():\n",
    "    \n",
    "   \n",
    "    query_str = query.get()\n",
    "    res1=data.iloc[search(query_str,inverted_index,data['Content'])]\n",
    "    docs.set(res1.iloc[:, 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "image=Image.open('D://src2_ir.png')\n",
    "image=image.resize((750,250))\n",
    "photo2=ImageTk.PhotoImage(image)\n",
    "Label(root,image=photo2).place(x=0,y=0)\n",
    "\n",
    "search_entry=Entry(root,width=55,borderwidth=2,font=(\"calibri\",18),justify='right',textvariable=query)\n",
    "search_entry.place(x=60,y=220)\n",
    "image=Image.open('D://src5_ir.png')\n",
    "image=image.resize((40,30))\n",
    "photo3=ImageTk.PhotoImage(image)\n",
    "search_btn=Button(root,width=40,height=30,image=photo3,command=search2)\n",
    "search_btn.place(x=15,y=218)\n",
    "frame1=Frame(root,bg=\"#ffffff\",width=700,height=400)\n",
    "frame1.place(x=10,y=270)\n",
    "\n",
    "Label(frame1, bg=\"#ffffff\", fg=\"#000000\", textvariable=docs, font=(\"calibri\", 8)).place(x=400, y=10)\n",
    "\n",
    "Button(frame1,width=17,height=2,text=\"Ranking\",bg='#cce7ec',command=ranking, font=(\"calibri\", 12)).place(x=320,y=240)\n",
    "\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec679183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
